#  Mythryl-Yacc Parser Generator (c) 1989 Andrew W. Appel, David R. Tarditi 

# base.api: Base api file for Mythryl-Yacc.  This file contains signatures
# that must be loaded before any of the files produced by Mythryl-Yacc are loaded


#  STREAM: api for a lazy stream.

api STREAM =
 api type stream( A_xa )
     my streamify:  (unit -> '_a) -> stream( '_a )
     my cons:  '_a * '_a stream -> stream( '_a )
     my get:  '_a stream -> '_a * stream( '_a )
 end

# LR_TABLE: api for an LR Table.
#
# The list of actions and gotos passed to make_lr_table must be ordered by state
# number. The values for state 0 are the first in the list, the values for
#  state 1 are next, etc.


api LR_TABLE =
    api
        enum Pairlist (X,Y) = EMPTY
                              | PAIR of X * Y * pairlist (X,Y)
	enum State = STATE of int
	enum Term = T of int
	enum Nonterm = NT of int
	enum Action = SHIFT of state
			| REDUCE of int
			| ACCEPT
			| ERROR
	type Table
	
	my state_count:  table -> int
	my rule_count:  table -> int
	my describe_actions:  table -> state ->
				 pairlist( term, action ) * action
	my describe_goto:  table -> state -> pairlist( nonterm, state )
	my action:  table -> state * term -> action
	my goto:  table -> state * nonterm -> state
	my initial_state:  table -> state
	exception Goto of state * nonterm

	my make_lr_table:  { actions:  ( pairlist( term, action) * action) array,
			   gotos:    array( pairlist( nonterm, state) ),
			   state_count:  int, rule_count:  int,
			   initial_state:  state
                          } -> table
    end

# TOKEN: api revealing the internal package of a token. This api
# TOKEN distinct from the api { parser name }_TOKENS produced by Mythryl-Yacc.
# The { parser name }_TOKENS structures contain some types and functions to
#  construct tokens from values and positions.
#
# The representation of token was very carefully chosen here to allow the
# typeagnostic parser to work without knowing the types of semantic values
# or line numbers.
#
# This has had an impact on the TOKENS package produced by Mythryl-Yacc, which
# is a package parameter to lexer functors.  We would like to have some
# type Token(X) which functions to construct tokens would create.  A
# constructor function for a integer token might be
#
#	  INT: int * X * X -> token(X)
# 
# This is not possible because we need to have tokens with the representation
# given below for the typeagnostic parser.
#
# Thus our constructur functions for tokens have the form:
#
#	  INT: int * X * X -> token( Semantic_Value, X )
#
# This in turn has had an impact on the api that lexers for Mythryl-Yacc
# must match and the types that a user must declare in the user declarations
# section of lexers.


api TOKEN =
    api
	package lr_table:  LR_TABLE
        enum Token (X,Y) = TOKEN of lr_table::term * (X * Y * Y)
	my sameToken:  token( X, Y) * token( X, Y ) -> bool
    end

#  LR_PARSER: api for a typeagnostic LR parser 

api LR_PARSER =
    api
	package stream: STREAM
	package lr_table:  LR_TABLE
	package token:  TOKEN

	sharing lr_table = token::lr_table

	exception PARSE_ERROR

	my parse:  { table:  lr_table::table,
		     lexer:  Stream::stream( token::Token( '_b, '_c) ),
		     arg: A_arg,
		     saction:  int *
			       '_c *
				 List( lr_table::state * ('_b * '_c * '_c)) * 
				A_arg ->
				     lr_table::nonterm *
				     ('_b * '_c * '_c) *
				     ( List( lr_table::state * ('_b * '_c * '_c))),
		     void:  '_b,
		     ec:  { is_keyword:  lr_table::term -> bool,
			    no_shift:  lr_table::term -> bool,
			    preferred_subst:  lr_table::term -> List( lr_table::term ),
			    preferred_insert:  lr_table::term -> bool,
			    errtermvalue:  lr_table::term -> '_b,
			    show_terminal:  lr_table::term -> string,
			    terms: List( lr_table::term ),
			    error:  string * '_c * '_c -> unit
			   },
		     lookahead:  int  #  max amount of lookahead used in 
				      #  error correction 
			} -> '_b *
			     (Stream::stream( token::Token('_b, '_c) ) )
    end

# Lexer: a api that most lexers produced for use with Mythryl-Yacc's
# output will match.  The user is responsible for declaring type Token,
# type Source_Position, and type Semantic_Value in the user_declarations section of a lexer.
#
# Note that type Token is abstract in the lexer.  This allows Mythryl-Yacc to
# create a TOKENS api for use with lexers produced by Mythryl-Lex that
# treats the type Token abstractly.  Lexers that are functors parametrized by
# a 'tokens' package matching a 'Tokens' api cannot examine the package
# of tokens.


api Lexer =
   api
       package user_declarations :
	   api
	        type Token( X, Y )
		type Source_Position
		type Semantic_Value
	   end
	my make_lexer:  (int -> string) -> unit -> 
         user_declarations::Token (user_declarations::Semantic_Value, user_declarations::Source_Position)
   end

# ARG_LEXER: the %arg option of Mythryl-Lex allows users to produce lexers which
#   also take an argument before yielding a function from unit to a token


api ARG_LEXER =
   api
       package user_declarations :
	   api
	        type Token( X, Y )
		type Source_Position
		type Semantic_Value
		type Arg
	   end
	my make_lexer:  (int -> string) -> user_declarations::Arg -> unit -> 
         user_declarations::Token (user_declarations::Semantic_Value, user_declarations::Source_Position)
   end

# Parser_Data: the api of parser_data structures in { parser name } lr_vals_fn
# produced by  Mythryl-Yacc.  All such structures match this api.  
#
# The { parser name } lr_vals_fn produces a package which contains all the values
# except for the lexer needed to call the typeagnostic parser mentioned
# before.

api Parser_Data =
   api
        #  the type of line numbers 

	type Source_Position

	#  the type of semantic values 

	type Semantic_Value

         #  the type of the user-supplied argument to the parser 
 	type Arg
 
	# the intended type of the result of the parser.  This value is
	# produced by applying extract from the package 'actions' to the
	# final semantic value resultiing from a parse.


	type Result

	package lr_table:  LR_TABLE
	package token:  TOKEN
	sharing token::lr_table = lr_table

	# package 'actions' contains the functions which mantain the
	# semantic values stack in the parser.  Void is used to provide
	# a default value for the semantic stack.

	package actions:  
	  api
	      my actions:  int * Source_Position *
		    List( lr_table::state * (Semantic_Value * Source_Position * Source_Position)) * Arg ->
		         lr_table::nonterm * (Semantic_Value * Source_Position * Source_Position) *
			 ( List( lr_table::state * (Semantic_Value * Source_Position * Source_Position) ) )
	      my void:  Semantic_Value
	      my extract:  Semantic_Value -> result
	  end

	# package ec contains information used to improve error
	# recovery in an error-correcting parser

	package ec :
	   api
     my is_keyword:  lr_table::term -> bool
	     my no_shift:  lr_table::term -> bool
	     my preferred_subst:  lr_table::term -> List( lr_table::term )
	     my preferred_insert:  lr_table::term -> bool
	     my errtermvalue:  lr_table::term -> Semantic_Value
	     my show_terminal:  lr_table::term -> string
	     my terms: List(  lr_table::term )
	   end

	#  table is the LR table for the parser 

	my table:  lr_table::table
    end

# api PARSER is the api that most user parsers created by 
# Mythryl-Yacc will match.


api PARSER =
    api
        package token:  TOKEN
	package stream:  STREAM
	exception PARSE_ERROR

	#  type Source_Position is the type of line numbers 

	type Source_Position

	#  type Result is the type of the result from the parser 

	type Result

         #  the type of the user-supplied argument to the parser 
 	type Arg
	
	# type Semantic_Value is the type of semantic values for the semantic value
	# stack


	type Semantic_Value

	#  my make_lexer is used to create a stream of tokens for the parser 

	my make_lexer:  (int -> string) ->
			  Stream::stream( token::Token( Semantic_Value, Source_Position) )

	# my parse takes a stream of tokens and a function to print
	# errors and returns a value of type Result and a stream containing
	# the unused tokens


	my parse:  int * ( Stream::stream( token::Token (Semantic_Value, Source_Position) ) ) *
		    (string * Source_Position * Source_Position -> unit) * Arg ->
				result *  Stream::stream( token::Token( Semantic_Value, Source_Position) )

	my sameToken:   token::Token( Semantic_Value, Source_Position) * token::Token (Semantic_Value, Source_Position)  ->
				bool
     end

# api ARG_PARSER is the api that will be matched by parsers whose
#  lexer takes an additional argument.


api ARG_PARSER = 
    api
        package token:  TOKEN
	package stream:  STREAM
	exception PARSE_ERROR

	type Arg
	type Lex_Arg
	type Source_Position
	type Result
	type Semantic_Value

	my make_lexer:  (int -> string) -> Lex_Arg ->
			  Stream::stream( token::Token (Semantic_Value, Source_Position) )
	my parse:  int * ( Stream::stream( token::Token( Semantic_Value, Source_Position ) ) ) *
		    (string * Source_Position * Source_Position -> unit) * Arg ->
				result *  Stream::stream( token::Token( Semantic_Value, Source_Position ) )

	my sameToken:   token::Token( Semantic_Value, Source_Position) *  token::Token( Semantic_Value, Source_Position) ->
				bool
     end

#  Mythryl-Yacc Parser Generator (c) 1989 Andrew W. Appel, David R. Tarditi 

# Stream: a package implementing a lazy stream.  The api STREAM
# is found in base.api

abstraction Stream:  STREAM =
pkg
   enum str X = EVAL   of X * Ref( str(X) )
               | UNEVAL of (unit -> X)

   type stream(X) = Ref( str(X) )

   fun get (ref (EVAL t)) = t
     | get (s as ref (UNEVAL f)) = 
	    let t = (f(), ref (UNEVAL f)) in s := EVAL t; t end

   fun streamify f = ref (UNEVAL f)
   fun cons (a, s) = ref (EVAL (a, s))

end;
#  Mythryl-Yacc Parser Generator (c) 1989 Andrew W. Appel, David R. Tarditi 

package lr_table:  LR_TABLE = 
    pkg
	use Array List
	infix 9 sub
	enum pairlist (X,Y) = EMPTY
			     | PAIR of X * Y * pairlist (X,Y)
	enum term = T of int
	enum nonterm = NT of int
	enum state = STATE of int
	enum action = SHIFT of state
			| REDUCE of int #  rulenum from grammar 
			| ACCEPT
			| ERROR
	exception Goto of state * nonterm
	type table = { states: int, rules:  int, initial_state: state,
		      action:  array( pairlist( term, action ) * action),
		      goto:    array( pairlist( nonterm, state ) ) }
	state_count = \\ ( { states,...} : table) => states
	rule_count = \\ ( { rules,...} : table) => rules
	describe_actions =
	   \\ ( { action,...} : table) => 
	           \\ (STATE s) => action sub s
	describe_goto =
	   \\ ( { goto,...} : table) =>
	           \\ (STATE s) => goto sub s
	fun findTerm (T term, row, default) =
	    let fun find (PAIR (T key, data, r)) =
		       if key < term then find r
		       else if key==term then data
		       else default
		   | find EMPTY = default
	    in find row
	    end
	fun findNonterm (NT nt, row) =
	    let fun find (PAIR (NT key, data, r)) =
		       if key < nt then find r
		       else if key==nt then THE data
		       else NULL
		   | find EMPTY = NULL
	    in find row
	    end
	action = \\ ( { action,...} : table) =>
		\\ (STATE state, term) =>
		  let my (row, default) = action sub state
		  in findTerm (term, row, default)
		  end
	goto = \\ ( { goto,...} : table) =>
			\\ (a as (STATE state, nonterm)) =>
			  case findNonterm (nonterm, goto sub state)
			  of THE state => state
			   | NULL => raise (Goto a)
	initial_state = \\ ( { initial_state,...} : table) => initial_state
	make_lr_table = \\ { actions, gotos, initial_state, state_count, rule_count } =>
	     ( { action=actions, goto=gotos,
	       states=state_count,
	       rules=rule_count,
               initial_state=initial_state } : table)
end;
#  Mythryl-Yacc Parser Generator (c) 1989 Andrew W. Appel, David R. Tarditi 

# package_macro join_fn creates a user parser by putting together a Lexer package,
# an LrValues package, and a typeagnostic parser package.  Note that
# the Lexer and LrValues package must share the type Source_Position (i.e. the type
# of line numbers), the type svalues for semantic values, and the type
# of tokens.


package_macro join_fn (

             package Lex:  Lexer
	     package parser_data: Parser_Data
	     package LrParser:  Lr_Parser
	     sharing parser_data::lr_table = LrParser::lr_table
	     sharing parser_data::token = LrParser::token
	     sharing Lex::user_declarations::Semantic_Value = parser_data::Semantic_Value
	     sharing Lex::user_declarations::Source_Position = parser_data::Source_Position
	     sharing Lex::user_declarations::Token = parser_data::token::Token)
		 : PARSER =
pkg
    package token = parser_data::token
    package stream = LrParser::Stream
 
    exception PARSE_ERROR = LrParser::PARSE_ERROR

    type Arg = parser_data::Arg
    type Source_Position = parser_data::Source_Position
    type Result = parser_data::Result
    type Semantic_Value = parser_data::Semantic_Value
    make_lexer = LrParser::Stream::streamify o Lex::make_lexer
    parse = \\ (lookahead, lexer, error, arg) =>
	(\\ (a, b) => (parser_data::actions::extract a, b))
     (LrParser::parse { table = parser_data::table,
	        lexer=lexer,
		lookahead=lookahead,
		saction = parser_data::actions::actions,
		arg=arg,
		void= parser_data::actions::void,
	        ec = { is_keyword = parser_data::ec::is_keyword,
		      no_shift = parser_data::ec::no_shift,
		      preferred_subst = parser_data::ec::preferred_subst,
		      preferred_insert= parser_data::ec::preferred_insert,
		      errtermvalue = parser_data::ec::errtermvalue,
		      error=error,
		      show_terminal = parser_data::ec::show_terminal,
		      terms = parser_data::ec::terms }}
      )
     sameToken = token::sameToken
end

# package_macro join_with_arg_fn creates a variant of the parser package produced 
# above.  In this case, the make_lexer take an additional argument before
# yielding a value of type unit -> token( Semantic_Value, Source_Position )


package_macro join_with_arg_fn (package Lex:  Arg_Lexer
	     package parser_data: Parser_Data
	     package LrParser:  Lr_Parser
	     sharing parser_data::lr_table = LrParser::lr_table
	     sharing parser_data::token = LrParser::token
	     sharing Lex::user_declarations::Semantic_Value = parser_data::Semantic_Value
	     sharing Lex::user_declarations::Source_Position = parser_data::Source_Position
	     sharing Lex::user_declarations::Token = parser_data::token::Token)
		 : ARG_PARSER  =
pkg
    package token = parser_data::token
    package stream = LrParser::Stream

    exception PARSE_ERROR = LrParser::PARSE_ERROR

    type Arg = parser_data::Arg
    type Lex_Arg = Lex::user_declarations::Arg
    type Source_Position = parser_data::Source_Position
    type Result = parser_data::Result
    type Semantic_Value = parser_data::Semantic_Value

    make_lexer = \\ s => \\ arg =>
		 LrParser::Stream::streamify (Lex::make_lexer s arg)
    parse = \\ (lookahead, lexer, error, arg) =>
	(\\ (a, b) => (parser_data::actions::extract a, b))
     (LrParser::parse { table = parser_data::table,
	        lexer=lexer,
		lookahead=lookahead,
		saction = parser_data::actions::actions,
		arg=arg,
		void= parser_data::actions::void,
	        ec = { is_keyword = parser_data::ec::is_keyword,
		      no_shift = parser_data::ec::no_shift,
		      preferred_subst = parser_data::ec::preferred_subst,
		      preferred_insert= parser_data::ec::preferred_insert,
		      errtermvalue = parser_data::ec::errtermvalue,
		      error=error,
		      show_terminal = parser_data::ec::show_terminal,
		      terms = parser_data::ec::terms }}
      )
    sameToken = token::sameToken
end;
#  Mythryl-Yacc Parser Generator (c) 1989 Andrew W. Appel, David R. Tarditi 

# parser.pkg:  This is a parser driver for LR tables with an error-recovery
# routine added to it.  The routine used is described in detail in this
# article:
#
#	'A Practical Method for LR and LL Syntactic Error Diagnosis and
#	 Recovery', by M. Burke and G. Fisher, ACM Transactions on
#	 Programming Langauges and Systems, Vol. 9, No. 2, April 1987,
#	 pp. 164-197.
#
#  This program is an implementation is the partial, deferred method discussed
#  in the article.  The algorithm and data structures used in the program
#  are described below.  
#
#  This program assumes that all semantic actions are delayed.  A semantic
#  action should produce a function from unit -> value instead of producing the
#  normal value.  The parser returns the semantic value on the top of the
#  stack when accept is encountered.  The user can deconstruct this value
#  and apply the unit -> value function in it to get the answer.
#
#  It also assumes that the lexer is a lazy stream.
#
#  Data Structures:
#  ----------------
#	
#	* The parser:
#
#	   The state stack has the type
#
#		  List( state * (semantic value * line # * line #))
#
#	   The parser keeps a queue of (state stack * lexer pair).  A lexer pair
#	 consists of a terminal * value pair and a lexer.  This allows the 
#	 parser to reconstruct the states for terminals to the left of a
#	 syntax error, and attempt to make error corrections there.
#
#	   The queue consists of a pair of lists (x, y).  New additions to
#	 the queue are cons'ed onto y.  The first element of x is the top
#	 of the queue.  If x is NIL, then y is reversed and used
#	 in place of x.
#
#    Algorithm:
#    ----------
#
#	* The steady-state parser:  
#
#	    This parser keeps the length of the queue of state stacks at
#	a steady state by always removing an element from the front when
#	another element is placed on the end.
#
#	    It has these arguments:
#
#	   stack: current stack
#	   queue: value of the queue
#	   lex_pair ((terminal, value), lex stream)
#
#	When SHIFT is encountered, the state to shift to and the value are
#	are pushed onto the state stack.  The state stack and lex_pair are
#	placed on the queue.  The front element of the queue is removed.
#
#	When REDUCTION is encountered, the rule is applied to the current
#	stack to yield a triple (nonterm, value, new stack).  A new
#	stack is formed by adding (goto (top state of stack, nonterm), value)
#	to the stack.
#
#	When ACCEPT is encountered, the top value from the stack and the
#	lexer are returned.
#
#	When an ERROR is encountered, fix_error is called.  FixError
#	takes the arguments to the parser, fixes the error if possible and
#        returns a new set of arguments.
#
#	* The distance-parser:
#
#	This parser includes an additional argument distance.  It pushes
#	elements on the queue until it has parsed distance tokens, or an
#	ACCEPT or ERROR occurs.  It returns a stack, lexer, the number of
#	tokens left unparsed, a queue, and an action option.



api FIFO = 
  api type queue(X)
      my empty:  queue(X)
      exception EMPTY
      my get:  queue(X) -> X * queue(X)
      my put:  X * queue(X) -> queue(X)
  end

/* drt (12/15/89) -- the package_macro should be used in development work, but
   it wastes space in the release version.

package_macro parser_gen_fn (package lr_table:  LR_TABLE
		  package stream:  STREAM) : LR_PARSER =
*/

abstraction LrParser:  LR_PARSER =
   pkg
      package lr_table = lr_table
      package stream = Stream

      package token:  TOKEN =
	pkg
	    package lr_table = lr_table
	    enum Token (X,Y) = TOKEN of lr_table::term * (X * Y * Y)
	    sameToken = \\ (TOKEN (t, _), TOKEN (t', _)) => t=t'
        end

      use lr_table
      use token

      DEBUG1 = FALSE
      DEBUG2 = FALSE
      exception PARSE_ERROR
      exception ParseImpossible of int

      abstraction Fifo:  FIFO =
        pkg
	  type queue(X)   = (List(X) *  List(X))
	  empty = (NIL, NIL)
	  exception EMPTY
	  fun get (a . x, y) = (a, (x, y))
	    | get (NIL, NIL) = raise EMPTY
	    | get (NIL, y) = get (reverse y, NIL)
 	  fun put (a, (x, y)) = (x, a . y)
        end

      type Element( X, Y ) = (state * (X * Y * Y))
      type stack( X, Y ) =  List( Element( X, Y ) )
      type lexv( X, Y ) = token( X, Y )
      type lexpair( X, Y ) =  lexv( X, Y ) *  Stream::stream(  lexv( X, Y) )
      type distance_parse( X, Y) =
		 lexpair( X, Y )  *
		 stack( X, Y)  * 
		 Fifo::queue( stack( X, Y)  * lexpair( X, Y ) )  *
		 int ->
		   lexpair( X, Y )  *
		   stack( X, Y )  * 
		   Fifo::queue( stack( X, Y )  * lexpair( X, Y ) )  *
		   int *
		   option( action )

      type Error_Recovery_Info( X, Y ) =
	 { is_keyword:  term -> bool,
          preferred_subst:  term -> List( term ),
	  preferred_insert:  term -> bool,
	  error:  string * Y * Y -> unit,
	  errtermvalue:  term -> X,
	  terms:  List( term ),
	  show_terminal:  term -> string,
	  no_shift:  term -> bool }

      local 
	 print = \\ s => output (std_out, s)
	 println = \\ s => (print s; print "\n")
	 showState = \\ (STATE s) => "STATE " ^ (makestring s)
      in
        fun printStack (stack:  stack( X, Y ), n: int) =
         case stack
           of (state, _) . rest =>
                 (print("\t" ^ makestring n ^ ": ");
                  println (showState state);
                  printStack (rest, n+1))
            | NIL => ()
                
        fun prAction show_terminal
		 (stack as (state, _) . _, next as (TOKEN (term, _), _), action) =
             (println "Parse: state stack:";
              printStack (stack, 0);
              print("       state="
                         ^ showState state	
                         ^ " next="
                         ^ show_terminal term
                         ^ " action="
                        );
              case action
                of SHIFT state => println ("SHIFT " ^ (showState state))
                 | REDUCE i => println ("REDUCE " ^ (makestring i))
                 | ERROR => println "ERROR"
		 | ACCEPT => println "ACCEPT")
        | prAction _ (_, _, action) = ()
     end

    # steadystate_parse: parser which maintains the queue of (state * lexvalues) in a
    #	steady-state.  It takes a table, show_terminal function, saction
    #	function, and fix_error function.  It parses until an ACCEPT is
    #	encountered, or an exception is raised.  When an error is encountered,
    #	fix_error is called with the arguments of parseStep (lexv, stack, and
    #	queue).  It returns the lexv, and a new stack and queue adjusted so
    #	that the lexv can be parsed
	
    steadystate_parse =
      \\ (table, show_terminal, saction, fix_error, arg) =>
	let prAction = prAction show_terminal
	    action = lr_table::action table
	    goto = lr_table::goto table
	    fun parseStep (args as
			 (lex_pair as (TOKEN (terminal, value as (_, leftPos, _)),
				      lexer
				      ),
			  stack as (state, _) . _,
			  queue)) =
	      let nextAction = action (state, terminal)
	          if DEBUG1 then prAction (stack, lex_pair, nextAction)
			  else ()
	      in case nextAction
		 of SHIFT s =>
		  let newStack = (s, value) . stack
		      newLexPair = Stream::get lexer
		      my (_, newQueue) =Fifo::get (Fifo::put((newStack, newLexPair),
							    queue))
		  in parseStep (newLexPair, (s, value) . stack, newQueue)
		  end
		 | REDUCE i =>
		     (case saction (i, leftPos, stack, arg)
		      of (nonterm, value, stack as (state, _) . _) =>
		          parseStep (lex_pair, (goto (state, nonterm), value) . stack,
				    queue)
		       | _ => raise (ParseImpossible 197))
		 | ERROR => parseStep (fix_error args)
		 | ACCEPT => 
			(case stack
			 of (_, (topvalue, _, _)) . _ =>
				let my (token, restLexer) = lex_pair
				in (topvalue, Stream::cons (token, restLexer))
				end
			  | _ => raise (ParseImpossible 202))
	      end
	    | parseStep _ = raise (ParseImpossible 204)
	in parseStep
	end

    #   distance_parse: parse until n tokens are shifted, or accept or
    #	error are encountered.  Takes a table, show_terminal function, and
    #	semantic action function.  Returns a parser which takes a lex_pair
    #	(lex result * lexer), a state stack, a queue, and a distance
    #	(must be > 0) to parse.  The parser returns a new lex-value, a stack
    #	with the nth token shifted on top, a queue, a distance, and action
    #	option.

    distance_parse =
      \\ (table, show_terminal, saction, arg) =>
	let prAction = prAction show_terminal
	    action = lr_table::action table
	    goto = lr_table::goto table
	    fun parseStep (lex_pair, stack, queue, 0) = (lex_pair, stack, queue, 0, NULL)
	      | parseStep (lex_pair as (TOKEN (terminal, value as (_, leftPos, _)),
				      lexer
				     ),
			  stack as (state, _) . _,
			  queue, distance) =
	      let nextAction = action (state, terminal)
	          if DEBUG1 then prAction (stack, lex_pair, nextAction)
			  else ()
	      in case nextAction
		 of SHIFT s =>
		  let newStack = (s, value) . stack
		      newLexPair = Stream::get lexer
		  in parseStep (newLexPair, (s, value) . stack,
			       Fifo::put((newStack, newLexPair), queue), distance - 1)
		  end
		 | REDUCE i =>
		    (case saction (i, leftPos, stack, arg)
		      of (nonterm, value, stack as (state, _) . _) =>
		         parseStep (lex_pair, (goto (state, nonterm), value) . stack,
				 queue, distance)
		      | _ => raise (ParseImpossible 240))
		 | ERROR => (lex_pair, stack, queue, distance, THE nextAction)
		 | ACCEPT => (lex_pair, stack, queue, distance, THE nextAction)
	      end
	   | parseStep _ = raise (ParseImpossible 242)
	in parseStep:  distance_parse( '_a, '_b )
	end

# mkFixError: function to create fix_error function which adjusts parser state
# so that parse may continue in the presence of an error

mkFixError = \\ ( { is_keyword, preferred_subst, terms, errtermvalue,
		      preferred_insert, no_shift,
		      show_terminal, error,...} :  Error_Recovery_Info( '_a, '_b),
		      distance_parse:   distance_parse('_a, '_b),
		      min_advance, max_advance) =>
  let fun FixError (lexv as (TOKEN (term, value as (_, leftPos, _)), _),
		   stack, queue) =
    let lexVList = map (\\ t => TOKEN (t, (errtermvalue t, leftPos, leftPos)))
		       terms
	if DEBUG2 then
			error("syntax error found at " ^ (show_terminal term),
			      leftPos, leftPos)
		else ()

	minDelta = 3

	#  pull all the state * lexv elements from the queue 

	stateList = 
	   let fun f q = let my (element, newQueue) = Fifo::get q
			 in element . (f newQueue)
			 end handle Fifo::EMPTY => NIL
	   in f queue
	   end

	# Now number elements of stateList,
	# giving distance from error token

	my (_, numStateList) = list::fold (\\ (a, (num, r)) => (num+1, (a, num) . r))
				stateList (0, NIL)

	# Represent the set of potential changes as a linked list.
        #
	#   Values of enum Change hold information about a potential change.
        #
	#  op = op to be applied
	#  pos = the # of the element in stateList that would be altered.
	#  distance = the number of tokens beyond the error token which the
	#    change allows us to parse.
	#  new = new terminal * value pair at that point
	#  orig = original terminal * value pair at the point being changed.

	enum op = INSERT | DELETE  | SUBST
	enum change (X,Y) = CHANGE of
	   { op:  op, pos:  int, distance:  int,
	    new:   lexv(X,Y), orig:  lexv (X,Y) }

	operToString = 
	       \\ INSERT => "INSERT "
		| SUBST  => "SUBST "
		| DELETE => "DELETE "

	 print_change = \\ c =>
	  let my CHANGE { op, distance, new=TOKEN (t, _),
			  orig=TOKEN (t', _), pos,...} = c
	  in (print ("{ distance= " ^ (makestring distance));
	      print (", orig = " ^ (show_terminal t'));
	      print (", new = " ^ (show_terminal t));
	      print (", op= " ^ (operToString op));
	      print (", pos= " ^ (makestring pos));
	      print "}\n")
	  end

	printChangeList = app print_change

/* parse: given a lex_pair, a stack, and the distance from the error
   token, return the distance past the error token that we are able to parse.*/

	fun parse (lex_pair, stack, queuePos:  int) =
	    let my (_, _, _, distance, action) =
		  distance_parse (lex_pair, stack, Fifo::empty, queuePos+max_advance+1)
	    in max_advance - distance - 1
	    end

#  foldStateList: accumulates results while scanning numStateList 


	fun foldStateList f start = list::fold f numStateList start

#  foldLexVList: accumulates results while scanning lexVList 

	fun foldLexVList f start = list::fold f lexVList start

# deleteFold: function which accumulates results of deleting the
# current terminal.  Does not delete the current terminal if that terminal
# cannot be shifted

	deleteFold =
		\\ (((stack, lex_pair as (orig as TOKEN (term, _), lexer)),
			queuePos), r) =>
		 if no_shift term then r
		 else
		   let my newLexPair as (new, _) = Stream::get lexer
		       distance = parse (newLexPair, stack, queuePos - 1)
		   in if distance >= min_advance then
			CHANGE { pos=queuePos, distance=distance, orig=orig,
				new=new, op=DELETE } . r
		      else r
		   end


# insertFold: accumulate results of trying to insert tokens before
# the current terminal

	insertFold =
	   \\ (((stack, lex_pair as (orig, lexer)), queuePos), r) =>
	    let lexer = Stream::cons lex_pair
	    in foldLexVList (\\ (newLexV, r) =>
		let distance = parse((newLexV, lexer), stack, queuePos+1)
		in if distance >= min_advance
			 then CHANGE { pos=queuePos, distance=distance, orig=orig,
					new=newLexV, op=INSERT } . r
			 else r
		end) r
	    end

# substFold: accumulate results of deleting the current terminal
# and then trying to insert tokens

	substFold =
	    \\ (((stack, lex_pair as (orig as TOKEN (term, _), lexer)), queuePos),
		r) =>
	      if no_shift term then r
	      else
		  foldLexVList (\\ (newLexV, r) =>
		   let distance = parse((newLexV, lexer), stack, queuePos)
		   in if distance >= min_advance then
			   CHANGE { pos=queuePos, distance=distance, orig=orig,
				  new=newLexV, op=SUBST } . r
		     else r
		   end) r

	changes = (foldStateList insertFold NIL) @
			  (foldStateList substFold NIL) @
				(foldStateList deleteFold NIL)

	findMaxDist = \\ l => 
	  fold (\\ (CHANGE { distance,...}, high) => max (distance, high)) l 0

#  maxDist: max distance past error taken that we could parse 

	maxDist = findMaxDist changes

#  sieve: keep only the elements of a list for which pred is TRUE 

	sieve = \\ pred => \\ l => 
	  fold (\\ (element, rest) => if pred element then element . rest else rest) l NIL

#  remove changes which did not parse maxDist tokens past the error token 

	changes = sieve (\\ CHANGE { distance=a,...} => a = maxDist) changes

#  Find preferred elements 

        preferredInsertChanges =
		sieve (\\ CHANGE { new=TOKEN (term, _), op=INSERT,...} => 
				 preferred_insert term
		        | _ => FALSE) changes

        preferredSubstChanges =
		sieve
		    (\\ CHANGE { new=TOKEN (t, _), orig=TOKEN (t', _),
				op=SUBST,...} =>
			  list::exists (\\ a => a =t) (preferred_subst t')
		      | _ => FALSE) changes

        if DEBUG2 then
	    (print "preferred insert:\n";
	     printChangeList preferredInsertChanges;
	     print "preferred subst:\n";
	     printChangeList preferredSubstChanges
	    ) else ()

# Remove keywords which don't meet the long parse check
# (min_advance+minDelta)

         changes =
	    sieve (\\ CHANGE { new=TOKEN (term, _), distance,...} =>
		(not (is_keyword term) or distance >= min_advance+minDelta))
			changes


         changes =
	       preferredInsertChanges @ (preferredSubstChanges @ changes)

         in case changes 
	     of (l as _ . _) =>
	        let fun print_msg (CHANGE { new=TOKEN (term, _), op,
					   orig=TOKEN (t', (_, leftPos, rightPos)),
					   ...} ) =
		     let s = 
		       case op
			 of DELETE => "deleting " ^ (show_terminal t')
			  | INSERT => "inserting " ^ (show_terminal term)
		          | SUBST => "replacing " ^ (show_terminal t') ^
				   " with " ^ (show_terminal term)
		     in error ("syntax error: " ^ s, leftPos, rightPos)
		     end
		   
		   a = 
		     (if length l > 1 and DEBUG2 then
			(print "multiple fixes possible; could fix it by:\n";
		 	 map print_msg l;
		 	 print "chosen correction:\n")
		      else ();
		      print_msg (hd l); (hd l))

		    # findNth: find nth queue entry from the error
		    # entry.  Returns the Nth queue entry and the  portion of
		    # the queue from the beginning to the nth-1 entry.  The
		    # error entry is at the end of the queue.
                    #
		    #	Examples:
                    #
		    #	queue = a b c d e
		    #    findNth 0 = (e, a b c d)
		    #	findNth 1 =  (d, a b c)


		    findNth = \\ n =>
		     let fun f (h . t, 0) = (h, reverse t)
			   | f (h . t, n) = f (t, n - 1)
			   | f (NIL, _) = let exception FindNth
					   in raise FindNth
					   end
		     in f (reverse stateList, n)
		     end
		
		    my CHANGE { pos, op, new=TOKEN (term, (value, _, _)),...} = a
		    my (last, queueFront) = findNth pos
		    my (stack, lex_pair as (orig, lexer)) = last
		    my TOKEN (_, (_, leftPos, rightPos)) = orig
 		    newLexV = TOKEN (term, (value, leftPos, rightPos))

		    newLexPair =
			case op
			of DELETE => Stream::get lexer
			 | SUBST => (newLexV, lexer)
			 | INSERT => (newLexV, Stream::cons lex_pair)

		    restQueue = 
		     Fifo::put((stack, newLexPair),
			      revfold Fifo::put queueFront Fifo::empty)

	 	   my (lex_pair, stack, queue, _, _) =
			distance_parse (newLexPair, stack, restQueue, pos)

	      in (lex_pair, stack, queue)
	      end
	  | NIL => (error("syntax error found at " ^ (show_terminal term),
			      leftPos, leftPos); raise PARSE_ERROR)
	end
     in FixError
     end

   parse = \\ { arg, table, lexer, saction, void, lookahead,
		   ec=ec as { show_terminal,...} :  Error_Recovery_Info( '_a, '_b ) } =>
	let distance = 15   #  Defer distance tokens 
	    min_advance = 1  #  must parse at least 1 token past error 
	    max_advance = max (lookahead, 0)#  max distance for parse check 
	    lex_pair = Stream::get lexer
	    my (TOKEN (_, (_, leftPos, _)), _) = lex_pair
	    start_stack = [(initial_state table, (void, leftPos, leftPos))]
	    start_queue = Fifo::put((start_stack, lex_pair), Fifo::empty)
	    distance_parse = distance_parse (table, show_terminal, saction, arg)
	    fix_error = mkFixError (ec, distance_parse, min_advance, max_advance)
	    steadystate_parse = steadystate_parse (table, show_terminal, saction, fix_error, arg)
	    fun loop (lex_pair, stack, queue, _, THE ACCEPT) =
		   steadystate_parse (lex_pair, stack, queue)
	      | loop (lex_pair, stack, queue, 0, _) = steadystate_parse (lex_pair, stack, queue)
	      | loop (lex_pair, stack, queue, distance, THE ERROR) =
		 let my (lex_pair, stack, queue) = fix_error (lex_pair, stack, queue)
		 in loop (distance_parse (lex_pair, stack, queue, distance))
		 end
	      | loop _ = let exception PARSE_INTERNAL
			 in raise PARSE_INTERNAL
			 end
	in loop (distance_parse (lex_pair, start_stack, start_queue, distance))
	end
 end;

/* drt (12/15/89) -- needed only when the code above is functorized

package LrParser = parser_gen_fn (package lr_table=lr_table
			     package stream=stream);
*/
