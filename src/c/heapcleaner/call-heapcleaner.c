// call-heapcleaner.c
//
// The main interface between the heapcleaner
// ("garbage collector") and the rest of the
// run-time system.
//
// We may be manually invoked from the Mythryl level via
//
//     src/lib/std/src/nj/heapcleaner-control.pkg
//
// We may be automatically invoked via the CHECKLIMIT macro in various allocation fns in
//
//     src/c/machine-dependent/prim.intel32.asm
// 
// and thence via   system_run_mythryl_task_and_runtime_eventloop/REQUEST_CLEANING   in
//
//     src/c/main/run-mythryl-code-and-runtime-eventloop.c
//
// to our   call_heapcleaner   entrypoint.
//
// More generally, we may be invoked via the heaplimit checks and heapcleaner calls generated by
//
//     src/lib/compiler/back/low/main/nextcode/pick-nextcode-fns-for-heaplimit-checks.pkg
//     src/lib/compiler/back/low/main/nextcode/emit-treecode-heapcleaner-calls-g.pkg

/*
Includes:
*/
#if NEED_HEAPCLEANER_PAUSE_STATISTICS		// Cleaner pause statistics are UNIX dependent.
    #include "system-dependent-unix-stuff.h"
#endif

#include "../mythryl-config.h"

#include <stdarg.h>
#include "runtime-base.h"
#include "runtime-configuration.h"
#include "get-multipage-ram-region-from-os.h"
#include "task.h"
#include "runtime-values.h"
#include "make-strings-and-vectors-etc.h"
#include "bigcounter.h"
#include "heap.h"
#include "runtime-globals.h"
#include "runtime-timer.h"
#include "heapcleaner-statistics.h"
#include "pthread-state.h"
#include "profiler-call-counts.h"

#ifdef C_CALLS											// C_CALLS is nowhere defined; it is referenced only in this file and in   src/c/lib/mythryl-callable-c-libraries-list.h
extern Val	mythryl_functions_referenced_from_c_code__global;				// mythryl_functions_referenced_from_c_code__global	def in   src/c/lib/ccalls/ccalls-fns.c
    //
    // This is a list of pointers into the C heap locations that hold
    // pointers to Mythryl functions. This list is not part of any Mythryl data
    // package(s).  (also see src/c/heapcleaner/heapclean-n-agegroups.c and src/c/lib/ccalls/ccalls-fns.c)
#endif


void   call_heapcleaner   (Task* task,  int level) {
    // ================
    //
    // Clean the heap. We always clean agegroup0. If level is greater than
    // 0, or if agegroup0 full after cleaning, we also clean
    // one or more additional agegroups.  (A minimum of 'level' agegroups are cleaned.)

    Val*  roots[ MAX_TOTAL_CLEANING_ROOTS ];							// Registers and globals.
    Val** roots_ptr = roots;
    Heap* heap;

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, PROF_MINOR_CLEANING );			// Remember that starting now CPU cycles are charged to the (minor) heapcleaner, not to the runtime or user code.
												// THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL is #defined      in	src/c/h/runtime-globals.h
												//  in terms of   this_fn_profiling_hook_refcell__global   from	src/c/main/construct-runtime-package.c

    #if NEED_PTHREAD_SUPPORT									// For background on the NEED_PTHREAD_SUPPORT stuff see the "Overview" comments in    src/lib/std/src/pthread.api
    if (pth__done_pthread_create__global) {
	//
	// Signal all pthreads to enter heapcleaner mode and
	// select a designated pthread to do the heapcleaning work.
	// That pthread returns and falls into the regular heapcleaning code;
	// the remainder block at a barrier until heapcleaning is complete:
												PTHREAD_LOG_IF ("initiating heapcleaning mode pid d=%d\n", task->pthread->pid);
	//
	if (!pth__start_heapcleaning( task )) {							// pth__start_heapcleaning		def in   src/c/heapcleaner/pthread-heapcleaner-stuff.c
	    //
	    // Return value was FALSE, so we're not the designated heapcleaner pthread,
	    // and our return from pth__start_heapcleaning means that the heapcleaning
	    // is already complete, so we can now resume execution of user code:
	    //
	    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, PROF_RUNTIME );			// Remember that starting now CPU cycles are charged to the runtime, not the heapcleaner.
	    //
	    return;
	}

	// At this point we know that
	// 
	//     1) We're the designated heapcleaner pthread.
	// 
	//     2) All other pthreads have now suspended execution of
	//        user code and are barrier-blocked waiting for us to
	//	  release them via the final pth__finish_heapcleaning()
	//        call below.
	// 
	// Consequently, at this point we can safely just fall
	// into the vanilla single-threaded heapcleaning code:
    }
    #endif

    note_when_heapcleaning_started( task->heap );						// note_when_heapcleaning_started	def in    src/c/heapcleaner/heapcleaner-statistics.h

    #ifdef C_CALLS
	*roots_ptr++ = &mythryl_functions_referenced_from_c_code__global;
    #endif

    #if NEED_PTHREAD_SUPPORT
    if (pth__done_pthread_create__global) {
        //
	// Get extra roots from pthreads that entered
	// through call_heapcleaner_with_extra_roots
	//
	for (int i = 0;   pth__extra_heapcleaner_roots__global[i] != NULL;   i++) {
	    //
	    *roots_ptr++ =  pth__extra_heapcleaner_roots__global[i];
	}
    }
    #endif

    // Note 4-5 C-level pointers into the Mythryl heap --
    // low-level special-case stuff like the signal handler,
    // runtime (pseudo-)package, pervasives etc:
    //
    for (int i = 0;  i < c_roots_count__global;  i++)   {					// c_roots_count__global		def in   src/c/main/construct-runtime-package.c
	//
	*roots_ptr++ =  c_roots__global[ i ];							// c_roots__global			def in   src/c/main/construct-runtime-package.c
    }

    #if NEED_PTHREAD_SUPPORT
    if (!pth__done_pthread_create__global) {
	//	
	*roots_ptr++ =  &task->link_register;
	*roots_ptr++ =  &task->argument;
	*roots_ptr++ =  &task->fate;
	*roots_ptr++ =  &task->current_closure;
	*roots_ptr++ =  &task->exception_fate;
	*roots_ptr++ =  &task->current_thread;
	*roots_ptr++ =  &task->callee_saved_registers[0];
	*roots_ptr++ =  &task->callee_saved_registers[1];
	*roots_ptr++ =  &task->callee_saved_registers[2];
	//
    } else {										// NEED_PTHREAD_SUPPORT
	//
	Pthread*  pthread;
	Task*     task;

	for (int j = 0;  j < MAX_PTHREADS;  j++) {
	    //
	    pthread = pthread_table__global[ j ];

	    task = pthread->task;
											PTHREAD_LOG_IF ("task[%d] alloc/limit was %x/%x\n", j, task->heap_allocation_pointer, task->heap_allocation_limit);
	    if (pthread->status != PTHREAD_IS_VOID) {
		//
		*roots_ptr++ =  &task->link_register;					// This line added 2011-11-15 CrT -- I think its lack was due to 15 years of bitrot.
		*roots_ptr++ =  &task->argument;
		*roots_ptr++ =  &task->fate;
		*roots_ptr++ =  &task->current_closure;
		*roots_ptr++ =  &task->exception_fate;
		*roots_ptr++ =  &task->current_thread;
		*roots_ptr++ =  &task->callee_saved_registers[0];
		*roots_ptr++ =  &task->callee_saved_registers[1];
		*roots_ptr++ =  &task->callee_saved_registers[2];
	    }
	}
    }
    #endif										// NEED_PTHREAD_SUPPORT

    *roots_ptr = NULL;

    heapclean_agegroup0( task, roots );							// heapclean_agegroup0	is from   src/c/heapcleaner/heapclean-agegroup0.c

    heap = task->heap;


    // If any generation-1 ilk is short on freespace,
    // commit to doing a multigeneration heapcleaning.
    //
    // We can skip this check if we're anyhow already
    // committed to    a multigeneration heapcleaning:
    //
    if (level == 0) {
        //
	Agegroup*	age1 =  heap->agegroup[0];
        //
	Val_Sized_Unt	size =   task->heap->agegroup0_buffer_bytesize;

	for (int i = 0;  i < MAX_PLAIN_ILKS;  i++) {
	    //
	    Sib* sib =  age1->sib[ i ];

	    if (sib_is_active( sib )							// sib_is_active		def in    src/c/h/heap.h
            &&  sib_freespace_in_bytes( sib ) < size					// sib_freespace_in_bytes	def in    src/c/h/heap.h
            ){
		level = 1;								// Commit to multigeneration heapcleaning.
		break;
	    }
	}
    }

    if (level > 0) {
        //
	*roots_ptr = NULL;

	ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, PROF_MAJOR_CLEANING );				// Remember that CPU cycles are charged to the heapcleaner (multigeneration pass).

	heapclean_n_agegroups( task, roots, level );							// heapclean_n_agegroups			def in   src/c/heapcleaner/heapclean-n-agegroups.c
    }

    // Reset the generation0 allocation pointers:
    //
    #if NEED_PTHREAD_SUPPORT										// NB: Currently is this is TRUE then we require that NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS also be TRUE.
    if (pth__done_pthread_create__global) {
	//
	pth__finish_heapcleaning( task );								// Multiple pthreads, so we must reset the generation-0 heap allocation pointers in each of them.
    } else {
	//
	task->heap_allocation_pointer	= heap->agegroup0_buffer;

	#if !NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS
	    //
	    task->heap_allocation_limit    = HEAP_ALLOCATION_LIMIT( heap );				// Set heap limit to obvious value.
	#else
	    reset_heap_allocation_limit_for_software_generated_periodic_events( task );			// Maybe set heap limit to artificially low value so as to regain control sooner to do software generated periodic event.
	#endif
    }
    #else	// Same as }else{ clause above:
    {
	task->heap_allocation_pointer	= heap->agegroup0_buffer;

	#if !NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS
	    //
	    task->heap_allocation_limit    = HEAP_ALLOCATION_LIMIT( heap );				// Set heap limit to obvious value.
	#else
	    reset_heap_allocation_limit_for_software_generated_periodic_events( task );			// Maybe set heap limit to artificially low value so as to regain control sooner to do software generated periodic event.
	#endif
    }
    #endif

    note_when_cleaning_completed();									// note_when_cleaning_completed	def in    src/c/heapcleaner/heapcleaner-statistics.h

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, PROF_RUNTIME );					// Remember that from here CPU cycles get charged to the runtime, not the heapcleaner.
}			                                             // fun call_heapcleaner


void   call_heapcleaner_with_extra_roots   (Task* task,  int level, ...)   {
    // =================================
    //
    // Clean with possible additional roots.  The list of
    // additional roots is NULL terminated.  We always clean agegroup0.
    // If level is greater than 0, or if agegroup 1 is full after cleaning
    // agegroup0, then we clean one or more additional agegroups.
    // At least 'level' agegroups are cleaned.
    //
    // NOTE: the multicore version of this may be BROKEN, since if a processor calls this
    // but isn't the collecting process, then THE EXTRA ROOTS ARE LOST.  XXX BUGGO FIXME

    Val*  roots[ MAX_TOTAL_CLEANING_ROOTS + MAX_EXTRA_HEAPCLEANER_ROOTS ];	// registers and globals
    Val** roots_ptr = roots;
    Val*  p;
    Heap* heap;

    va_list ap;

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, PROF_MINOR_CLEANING );					// Remember that CPU cycles after this get charged to the heapcleaner (generation0 pass).

    #if NEED_PTHREAD_SUPPORT
    if (pth__done_pthread_create__global) {
	//
														PTHREAD_LOG_IF ("initiating heapcleaning mode (with roots) pid d=%d\n", task->pthread->pid);
	va_start (ap, level);

	int we_are_the_designated_heapcleaner_pthread
	    =
	    pth__call_heapcleaner_with_extra_roots (task, ap);							// pth__call_heapcleaner_with_extra_roots	def in   src/c/heapcleaner/pthread-heapcleaner-stuff.c

	va_end(ap);

	if (!we_are_the_designated_heapcleaner_pthread)	{
	    //
	    // We are not the designated heapcleaner pthread, and our
	    // return from pth__start_heapcleaning means that the heapcleaning
	    // is already complete, so we can now resume execution of user code.
	    //
	    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, PROF_RUNTIME );					// Remember that from here CPU cycles are charged to the runtime, not the heapcleaner.
	    //
	    return;
	}

	// At this point we know that
	// 
	//     1) We're the designated heapcleaner pthread.
	// 
	//     2) All other pthreads have now suspended execution of
	//        user code and are barrier-blocked waiting for us to
	//	  release them via the final pth__finish_heapcleaning()
	//        call below.
	// 
	// Consequently, at this point we can safely just fall
	// into the vanilla single-threaded heapcleaning code:
    }
    #endif

    note_when_heapcleaning_started( task->heap );								// note_when_heapcleaning_started	def in    src/c/heapcleaner/heapcleaner-statistics.h

    #ifdef C_CALLS
	*roots_ptr++ = &mythryl_functions_referenced_from_c_code__global;
    #endif

    #if NEED_PTHREAD_SUPPORT
    if (pth__done_pthread_create__global) {
        // Get extra roots from pthreads that entered through call_heapcleaner_with_extra_roots.
        // Our extra roots were placed in pth__extra_heapcleaner_roots__global by pth__call_heapcleaner_with_extra_roots.
        //
	for (int i = 0;  pth__extra_heapcleaner_roots__global[i] != NULL;  i++) {
	    //
	    *roots_ptr++ =  pth__extra_heapcleaner_roots__global[ i ];
	}
    } else {
	//
        // Note extra roots from argument list:
	//
	va_start (ap, level);
	//
	while ((p = va_arg(ap, Val *)) != NULL) {
	    //
	    *roots_ptr++ = p;
	}
	va_end(ap);
    }
    #else  // Same as }else{ clause above:
    {
        // Note extra roots from argument list:
	//
	va_start (ap, level);
	//
	while ((p = va_arg(ap, Val *)) != NULL) {
	    //
	    *roots_ptr++ = p;
	}
	va_end(ap);
    }
    #endif						// NEED_PTHREAD_SUPPORT

    // Gather the roots:
    //
    for (int i = 0;  i < c_roots_count__global;  i++) {
	//
	*roots_ptr++ =  c_roots__global[ i ];
    }

    #if NEED_PTHREAD_SUPPORT
    if (pth__done_pthread_create__global) {
	//
	Task*     task;
	Pthread*  pthread;

	for (int j = 0;  j < MAX_PTHREADS;  j++) {
	    //
	    pthread = pthread_table__global[ j ];

	    task    = pthread->task;
														PTHREAD_LOG_IF ("task[%d] alloc/limit was %x/%x\n", j, task->heap_allocation_pointer, task->heap_allocation_limit);
	    if (pthread->status != PTHREAD_IS_VOID) {
		//
		*roots_ptr++ =  &task->link_register;					// This line added 2011-11-15 CrT -- I think its lack was due to 15 years of bitrot.
		*roots_ptr++ =  &task->argument;
		*roots_ptr++ =  &task->fate;
		*roots_ptr++ =  &task->current_closure;
		*roots_ptr++ =  &task->exception_fate;
		*roots_ptr++ =  &task->current_thread;
		*roots_ptr++ =  &task->callee_saved_registers[ 0 ];
		*roots_ptr++ =  &task->callee_saved_registers[ 1 ];
		*roots_ptr++ =  &task->callee_saved_registers[ 2 ];
	    }
	}

    } else {
	//
	*roots_ptr++ =  &task->link_register;						// This line added 2011-11-15 CrT -- I think its lack was due to 15 years of bitrot.
	*roots_ptr++ =  &task->argument;
	*roots_ptr++ =  &task->fate;
	*roots_ptr++ =  &task->current_closure;
	*roots_ptr++ =  &task->exception_fate;
	*roots_ptr++ =  &task->current_thread;
	*roots_ptr++ =  &task->callee_saved_registers[0];
	*roots_ptr++ =  &task->callee_saved_registers[1];
	*roots_ptr++ =  &task->callee_saved_registers[2];
    }
    #else	// Same as }else{ clause above.		// NEED_PTHREAD_SUPPORT
	//
	*roots_ptr++ =  &task->link_register;
	*roots_ptr++ =  &task->argument;
	*roots_ptr++ =  &task->fate;
	*roots_ptr++ =  &task->current_closure;
	*roots_ptr++ =  &task->exception_fate;
	*roots_ptr++ =  &task->current_thread;
	*roots_ptr++ =  &task->callee_saved_registers[0];
	*roots_ptr++ =  &task->callee_saved_registers[1];
	*roots_ptr++ =  &task->callee_saved_registers[2];
    #endif						// NEED_PTHREAD_SUPPORT

    *roots_ptr = NULL;

    heapclean_agegroup0( task, roots );			// heapclean_agegroup0	is from   src/c/heapcleaner/heapclean-agegroup0.c

    heap = task->heap;


    // If any generation-1 ilk is short on freespace,
    // commit to doing a multigeneration heapcleaning.
    //
    // We can skip this check if we're anyhow already
    // committed to    a multigeneration heapcleaning:
    //
    if (level == 0) {
        //
	Agegroup*	age1 =  heap->agegroup[0];
        //
	Val_Sized_Unt  size =  task->heap->agegroup0_buffer_bytesize;

	for (int i = 0;  i < MAX_PLAIN_ILKS;  i++) {
	    //
	    Sib* sib = age1->sib[ i ];
	    //
	    if (sib_is_active( sib )							// sib_is_active		def in    src/c/h/heap.h
            && (sib_freespace_in_bytes( sib ) < size)					// sib_freespace_in_bytes	def in    src/c/h/heap.h
            ){
		level = 1;
		break;
	    }
	}
    }

    if (level > 0) {
	//
	ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, PROF_MAJOR_CLEANING );				// Remember that CPU cycles are now being charged to the heapcleaner (multigeneration pass).

	heapclean_n_agegroups( task, roots, level );							// heapclean_n_agegroups			def in   src/c/heapcleaner/heapclean-n-agegroups.c
    }

    // Reset agegroup0 buffer:
    //
    #if NEED_PTHREAD_SUPPORT
    if (pth__done_pthread_create__global) {
        //
	pth__finish_heapcleaning( task );
    } else {
	task->heap_allocation_pointer	= heap->agegroup0_buffer;

	#if NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS
	    //
	    reset_heap_allocation_limit_for_software_generated_periodic_events( task );
	#else
	    task->heap_allocation_limit    = HEAP_ALLOCATION_LIMIT( heap );
	#endif
    }
    #else // Same as above }else{ case:
	task->heap_allocation_pointer	= heap->agegroup0_buffer;

	#if NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS
	    //
	    reset_heap_allocation_limit_for_software_generated_periodic_events( task );
	#else
	    task->heap_allocation_limit    = HEAP_ALLOCATION_LIMIT( heap );
	#endif
    #endif

    note_when_cleaning_completed();										// note_when_cleaning_completed	def in    src/c/heapcleaner/heapcleaner-statistics.h

    ASSIGN( THIS_FN_PROFILING_HOOK_REFCELL__GLOBAL, PROF_RUNTIME );
}														// fun call_heapcleaner_with_extra_roots



Bool   need_to_call_heapcleaner   (Task* task,  Val_Sized_Unt nbytes)   {
    // ========================
    //
    // This fun is called various places to guarantee that there are 'nbytes'
    // free in the generation-zero buffer.  The canonical calls are those in
    //
    //     src/c/main/run-mythryl-code-and-runtime-eventloop.c
    //
    // This function is also (ab)used to trigger period-event processing by
    // either setting the end-of-heap limit artificially low, or else by
    // setting SOFTWARE_GENERATED_PERIODIC_EVENTS_SWITCH_REFCELL__GLOBAL
    // to HEAP_TRUE.
    //
    // Check to see if a heapcleaning is required,
    // or if there is enough heap space
    // for nbytes worth of allocation.
    //	
    // Return TRUE, if the heapcleaner should be called,
    // FALSE otherwise.

// There was a #if NEED_PTHREAD_SUPPORT here but the logic was so complex I dropped it to simplify things... 2011-11-12 CrT
    if (pth__done_pthread_create__global) { // XYZZY PLUGH Changing this to (1 || ...) yields Fatal error:  bad chunk tag 19, chunk = 0x42c40064, tagword = 0x42c401cc
                                                 //                                                Fatal error:  bad chunk tag 19, chunk = 0x42c40064, tagword = 0x42c401cc
    #if NEED_PTHREAD_SUPPORT_FOR_SOFTWARE_GENERATED_PERIODIC_EVENTS
	//
	if ((((Punt)(task->heap_allocation_pointer)+nbytes) >= (Punt) HEAP_ALLOCATION_LIMIT( task->heap ))	// HEAP_ALLOCATION_LIMIT	is #defined in   src/c/h/heap.h
	|| (DEREF( SOFTWARE_GENERATED_PERIODIC_EVENTS_SWITCH_REFCELL__GLOBAL) == HEAP_TRUE))			// This appears to be set mainly (only?) in   src/c/heapcleaner/pthread-heapcleaner-stuff.c
	//													// although it is also exported to the Mythryl level via   src/lib/std/src/unsafe/software-generated-periodic-events.api
	     return TRUE;
	else return FALSE;
    #else
	//
	if (((Punt)(task->heap_allocation_pointer)+nbytes) >= (Punt) HEAP_ALLOCATION_LIMIT( task->heap ))	return TRUE;	// HEAP_ALLOCATION_LIMIT	is #defined in   src/c/h/heap.h
	else													return FALSE;
    #endif
    } else {
	if (((Punt)(task->heap_allocation_pointer)+nbytes) >= (Punt) HEAP_ALLOCATION_LIMIT( task->heap ))	return TRUE;	// HEAP_ALLOCATION_LIMIT	is #defined in   src/c/h/heap.h
	else													return FALSE;
    }
//    #endif
}


#if NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS

    void   reset_heap_allocation_limit_for_software_generated_periodic_events   (Task* task)   {
	// ==================================================================
	//
	// Reset the limit pointer according to the current polling frequency.

	int poll_frequency
	    = 
	    TAGGED_INT_TO_C_INT(DEREF(SOFTWARE_GENERATED_PERIODIC_EVENT_INTERVAL_REFCELL__GLOBAL));	// SOFTWARE_GENERATED_PERIODIC_EVENT_INTERVAL_REFCELL__GLOBAL is #defined in src/c/h/runtime-globals.h
													// in terms of software_generated_periodic_event_interval_refcell__global from src/c/main/construct-runtime-package.c
	Heap* heap =  task->heap;

	// Assumes heap_allocation_pointer has been reset:
	//
	task->real_heap_allocation_limit =  HEAP_ALLOCATION_LIMIT( heap );

	if (poll_frequency <= 0) {
	    //
	    task->heap_allocation_limit  = task->real_heap_allocation_limit;

	} else {

	    task->heap_allocation_limit  =  heap->agegroup0_buffer + poll_frequency * PERIODIC_EVENT_TIME_GRANULARITY_IN_NEXTCODE_INSTRUCTIONS;
	    //
	    task->heap_allocation_limit  =  (task->heap_allocation_limit > task->real_heap_allocation_limit)
		? task->real_heap_allocation_limit
		: task->heap_allocation_limit;
	}
    }
#endif						// NEED_SOFTWARE_GENERATED_PERIODIC_EVENTS


// COPYRIGHT (c) 1993 by AT&T Bell Laboratories.
// Subsequent changes by Jeff Prothero Copyright (c) 2010-2011,
// released under Gnu Public Licence version 3.






/*
##########################################################################
#   The following is support for outline-minor-mode in emacs.		 #
#  ^C @ ^T hides all Text. (Leaves all headings.)			 #
#  ^C @ ^A shows All of file.						 #
#  ^C @ ^Q Quickfolds entire file. (Leaves only top-level headings.)	 #
#  ^C @ ^I shows Immediate children of node.				 #
#  ^C @ ^S Shows all of a node.						 #
#  ^C @ ^D hiDes all of a node.						 #
#  ^HFoutline-mode gives more details.					 #
#  (Or do ^HI and read emacs:outline mode.)				 #
#									 #
# Local variables:							 #
# mode: outline-minor							 #
# outline-regexp: "[A-Za-z]"			 		 	 #
# End:									 #
##########################################################################
*/
